seed: 13

model:
  base: "bert-base-uncased"  # e.g. "TODBERT/TOD-BERT-JNT-V1"
  pooling_mode: "cls"  # "cls"; "mean"; "max"; "weightedmean"; "lasttoken";
  special_tokens: []  # e.g. ["[USR]", "[SYS]"]
  max_seq_length: null  # if null, then min(auto_model.config.max_position_embeddings, tokenizer.model_max_length)

target:
  losses: ["denoising-auto-encoder", "multiple-negatives-ranking", "cosine-similarity"]  # "softmax"; "denoising-auto-encoder"; "multiple-negatives-ranking"; "cosine-similarity";
  trainsets: ["data/dialogue.txt", "data/AllNLI_train.csv", "data/stsbenchmark_train.csv"]

contrastive_learning:
  label_pos: "entailment"
  label_neg: "contradiction"

evaluation:
  evaluations_per_epoch: 50  # how many evaluations to perform per epoch
  min_steps: 100  # minimum number of steps for evaluations to happend
  metric: "coorelation-score"  # "coorelation-score"  (Spearman correlation) or "f1-score", "accuracy", "recall", "precision" (sklearn classification_report metrics)
  metric_avg: "macro"  # "macro", "micro", "weighted" (ignore if not classification)
  devset: "data/stsbenchmark_dev.csv"
  testset: "data/stsbenchmark_test.csv"
  best_model_output_path: "outputs/test"

training:
  batch_size: 16
  num_epochs: 2
  warmup_pct: 0.1  # %
  learning_rate: 2e-5

checkpointing:
  saves_per_epoch: 0
  min_steps: 50  # minimum number of steps for checkpoint saving to happend
  total_limit: 0
  always_save_after_each_epoch: True
  path: "outputs/test/checkpoints"

wandb:
  project_name: null  # if null a defult name will be given using the provided parameters (traininset, model name, etc.)
  log_freq: 100

datasets:
  csv:
    column_name_sent1: "sent1"
    column_name_sent2: "sent2"
    column_name_ground_truth: "value"